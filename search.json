[{"title":"用ASAN平台构建质量防护网","url":"/2024/05/26/用ASAN平台构建质量防护网/","content":"\n在处理C/C++等低级编程语言时，内存泄漏是一个难以避免的挑战。我们常见的问题如数组越界、栈溢出、重复释放（double free）和堆溢出等情况层出不穷。得益于C语言的高效率，它至今仍是绝大多数嵌入式设备的运行基石。内存泄漏的后果可能非常严重，从程序崩溃到对嵌入式设备失去控制不一而足。更加难以追溯的是，内存泄漏可能导致程序在运行一段时间后出现异常行为，这被称为内存踩踏。定位内存踩踏问题非常困难，常需结合coredump文件分析和代码审查，耗费大量时间。\n\n为了拦截内存泄漏问题，通常成熟的公司里有多重防线：包括开发阶段的自我验证、代码提交前的检查（code check）、单元测试和代码审阅，以及交由测试人员进行的针对性测试。与可以在线升级的互联网产品不同，嵌入式设备的软件更新升级过程需要更多资源和严格验证。\n\n从减少内存泄漏的角度看，提高开发人员的编码规范意识和更细致的代码审查是人为层面的努力方向。而从流程和工具的角度，则可以引入严格而有效的自动化检查工具。例如，内存地址消毒器（AddressSanitizer）就是一个在开发测试阶段揭示内存错误的有力工具。它能发现一系列复杂的内存问题，如数组越界和多线程竞争等，尽管这可能会导致一定的运行时性能损失和资源占用。但相比之下，它带来的软件质量提升更为显著。\n\n现在假设我们有了编译携带ASAN的版本，并且在公司内部所有开发测试环境都运行着它。ASAN会在发生内存错误的时候（尽可能）打印出调用栈信息和发生问题的内存区块信息。默认会停止程序的运行（但是在实践中开发测试人员通常会设置ASAN_OPTIONS使得程序继续正常运行）。没有多少人会去有意识地查看是否有ASAN报告发生。在大型的软件团队内，角色分工明确，责任田明晰，可能有大量的测试环境供工程师们使用。那么这些ASAN报告就分散在不同的机器上。需要一定的训练才能理解（例如什么是全局缓冲区溢出global-buffer-overflow，什么是mmap）。这大大增加了将报告转化为问题单的难度。\n\n我曾经因为写了一段低级的代码导致内存泄漏。这次经历让我认识到了问题的严重性，并在之后找到并引入ASAN这个工具到我们的团队中以提高代码质量。然而，我发现尽管这个工具很有用，大多数团队成员却对它不够重视或根本不知道它的存在。因此，一种将所有ASAN日志集中在一处、易于访问和理解的方案开始在我脑海中形成。为了实现这一点，我和一位小伙伴开始着手开发这样一个平台。\n\n具体来说，这个平台不仅集中了所有的ASAN日志，还能根据业务领域将问题分配给责任人，确保各个问题都能被有效追踪到个人。该平台对相似的问题进行去重，记录了每个问题的类型和解决状态，极大地方便了问题的跟踪工作。技术栈上包括使用bash脚本定期上传日志文件至服务器、利用Python Flask搭建的Web服务器以及通过Streamlit创建的状态监控网页。\n\n这类的工作是know how的工作，知道如何在docker镜像中添加运行时需要的依赖，知道在哪里能够打桩上传脚本，知道如何对ASAN日志文件进行模式匹配从而提取出定位所需的最小信息。。这类的工作最开始的时候是一次做一个功能，做完了之后得到反馈和需求，再去优化。没想到最终成了一个质量防护网的创新。能给公司减少一些问题定位时的人力消耗，并且减少问题泄漏到生产环境的风险。","tags":["工作"]},{"title":"cron在线验证环境","url":"/2024/03/12/cron在线验证环境/"},{"title":"PN结","url":"/2024/03/12/PN结/"},{"title":"libhv代码走读1","url":"/2024/03/12/libhv代码走读1/","content":"\nlibhv 项目地址\n\n功能\n\n用途\n\n## 实现\n\n### 日志器\n\n### 多线程\n\n### 配置读取\n\n### 锁\n\n自旋锁\n\n读写锁\n\n定时锁\n\n## 缺点\n\njson解析慢\n\n"},{"title":"《程序员的README》读书笔记","url":"/2024/02/20/程序员的README-读书笔记/","content":"\n2024年春节的时候，读了这本《程序员的README》。当时写下的评价是如下\n\n非常推荐这本书，符合了一个优秀的readme应该具有的特性：\n1. 完整性。将软件工程师的职业体系，需要掌握的软硬能力列举出来。并且解释了工作中会遇到的各种类型活动如代码提交，代码评审，需求分析设计，on-call，敏捷开发，迭代开发，绩效评定。 \n2. 扩展性。在每章的加油站部分给了很多继续探索的建议。\n3. 可行性。给出了“安装”，“运行”的事例。如何上手，如何沟通，不要变得粗鲁，也不要接受被粗鲁对待。\n\n\n## 读书摘抄\n###  **2.1 学习如何学习**\n- 请每周都花一部分时间去阅读。可供阅读的内容有很多：团队文档、设计文档、代码、积压的任务票、书籍、论文和技术网站。不要试图一下子把所有东西都读完。请从团队文档和设计文档入手。这些文档会就事情是如何组合在一起的给你一个整体的概念。要特别注意那些关于如何权衡取舍和背景的讨论。接下来你就可以深入研究那几个与你最初任务相关的子系统了。  \n    \n- 去读源代码，因为它并不总是与设计文档相吻合！不要只读你自己的代码库，还要去阅读高质量的开源项目，特别是那些你使用的类库。不要像阅读小说一样从前到后地通读代码：请利用你的IDE来浏览代码。为关键的操作绘制控制流和状态图。仔细研究代码的数据结构和算法。注意那些临界值的处理。留意那些惯用写法和风格，也就是去学习“本地方言”(local dialect)。  \n###  **2.2 提出问题**\n- 限制你研究一个问题时预期花费的时间。  \n###  **4.2 关于日志的使用**\n- 输出日志信息对理解代码或调试一个小程序来说既简单又方便。对于复杂的应用程序，编程语言有精良的日志类库，让运维人员对要记录的内容和时间有更多的控制。运维人员可以通过修改日志级别来调节输出日志的总量，并控制日志格式。日志框架还可以注入上下文信息，诸如线程名、主机名、ID，你可以在调试的时候使用这些信息。日志框架与日志管理系统可以很好地配合，这种系统可以聚集日志信息，所以运维人员可以过滤并搜索它们。  \n###  **第五十章 7.2 当你的代码被评审时**\n- 不要试图让你的队友在预排会议中实际地进行代码评审，参加者应该把他们的评论留到未来真正的代码评审环节。预排会议的目的是帮助你的团队理解为什么要提出修改，并给他们一个良好的心理模型，以便他们可以自行去进行详细的代码评审。 7.2.5 不要太在意  \n    \n- 保持同理心，但不要容忍粗鲁\n###  **7.3 评审别人的代码时**\n- 要专注于那些你可以从中学习的修改和你熟悉的代码。  \n###  **7.5 升级加油站**\n- 《开发者代码评审指南》(“Code Review Developer Guide”)  \n    \n- 《高难度谈话Ⅱ：感恩反馈》（Thanks for the Feedback: The Science and Art of Receiving Feedback Well\n###  **8.1 软件交付流程**\n- 分支策略决定了代码变更的提交位置以及发布代码的维护方式。正确的分支策略将使软件交付变得简单和可预测，而错误的策略将使交付变成与流程本身的缠斗。  \n###  **8.2 分支策略**\n- 一个发布分支也被切了下来，开发人员决定把这个bug的修正内容转移提交(cherry-pick)到发布版-1.0(release-1.0)版本中。 [插图] 图8-2 基于主分支的开发模式 只有当各分支可以快速合并到主分支时，基于主分支的开发模式的效果才是最好的，如果不是在几小时内，也应该在几天内合并到主分支，并且不在开发人员之间共享。频繁地合并被称为持续集成(CI)。CI可降低风险，因为代码上的变化会迅速传递给所有的开发人员，使他们彼此之间不太可能有很大的分歧。  \n    \n- 开发人员的代码库保持同步，可以防止潜在的最后一分钟的集成障碍，并尽早暴露出错误和不兼容的情况。作为一种代价，主分支中的bug会拖累所有的开发者。为了防止代码破损，在一个分支被合并到主分支上之前，要运行快速的自动化测试来验证其是否可以通过。团队通常有明确的流程来应对破损的主分支，一般的期望是主分支应该总是可以发布的，而且发布往往相当频繁。  \n###  **9.1 On-Call的工作方式**\n- On-Call的开发人员必须对事故分流、缓解症状和最终解决。  \n###  **9.4 提供支持**\n- 由于你“真正的”工作是编程，参与到支持工作中可能会让你分心。把支持工作看成一次学习的机会，你会看到你团队的软件是如何在现实世界中被使用的，以及它失败或使用户感到困惑的方式。回答支持请求会把你带到你不熟悉的代码部分，你必须努力思考和实验。你会注意到导致问题发生的模式，这将帮助你在未来创建更好的软件。参与支持工作的轮岗会使你成为一名更好的工程师。另外，你可以帮助别人、建立关系和自己的声誉。快速、高质量的支持响应不会被忽视。  \n###  **10.2 关于设计的思考**\n- “Manager’s Schedule, Maker’s Schedule”\n###  **10.3 撰写设计文档**\n- 设计文档是一种工具，可以帮助你思考、获得反馈、让你的团队了解情况、培养新的工程师，并推动项目规划。  \n    \n- 写得清晰会让你的生活更轻松。写作是一种有损的信息传递方式：你把你的想法写下来，而你的队友则在他们的头脑中不完全地重建你的想法  \n###  **12.4 站会**\n- 站会是一种定期的系统检查——看一眼你的汽车仪表盘，以确保你有汽油，而且其神秘的“检查引擎”灯并没有亮起  \n###  **12.9 升级加油站**\n- 敏捷软件开发宣言》有一个额外的页面，叫作《〈敏捷宣言〉背后的原则》（“Principles Behind the Agile Manifesto”，可以在其官方网页找到全文）。看一下这些原则，以了解更多关于这种软件开发哲学的细节。  \n###  **13.1 管理者是做什么的**\n- 工程经理的工作是关于人、产品和流程的。管理者们构建团队、指导和培养工程师，并进行人际关系的动态管理，工程经理还计划和协调产品的开发。  \n###  **13.2 沟通、目标与成长**\n- 大局观：你对公司的方向有什么疑问？你对组织变革有什么疑问？ 反馈：我们可以在哪些方面做得更好？你对团队的计划流程有什么看法？你最大的技术难题是什么？你希望你能做什么而你却做不到？你最大的问题是什么？公司的最大问题是什么？你或团队中的其他人遇到了什么阻碍？ 职业生涯：你的管理者对你都有哪些职业建议？你有哪些可以改进的地方？你希望自己有哪些技能？你的长期目标是什么，你觉得你在这些目标上的进展如何？ 个人事务：你的生活中有什么新鲜事？你的管理者应该注意你的哪些个人问题？  \n    \n- 看作一次机会：回顾你所取得的成果、谈论你接下来要做的事情、公开承认错失、制定下一年的成长计划，并向你的管理者提供反馈。你不应该对你的绩效考核的反馈感到惊讶，如果你感到惊讶，请与你的管理者讨论沟通上的偏差。一份成功的绩效考核应该给你具体的行动来实现你的目标。  \n###  **第一四章 14.2 职业生涯建议**\n\n- 的职业生涯是一场马拉松，而不是短跑冲刺——你有几十年的时间。给自己定下节奏，享受这段旅程吧！  \n\n## 读后记录\n软件工程是一个系统性工程。作为一个刚入门的新兵，要学习的很多，职业发展地图还有很多要走的。从单点的解决问题到为持续高质量交付做努力。成一些事，持续成事。要做的很多。有这些：对职场的认识，时间管理，工作效率，工作流程的优化。再多看看书\n\n## 下一步阅读\n《软件设计的哲学》\n《优雅谜题：工程管理的系统》\n《向上管理：如何晋升，如何在工作中获胜以及如何与不同类型的上司一同获取成功》\n《实现领域驱动设计》","tags":["读书"]},{"title":"20240218-夜间散步时的自说自话","url":"/2024/02/18/20240218-散步时的自说自话/","content":"\n讲话人1 - 0:00\n\n一个人在路上瞎说点什么，然后拿手表来记录，吧刚刚说了一句话，这是一棵树，他还活着那是两个人，他们坐在地上今天不知道怎么的，情绪在8点多，达到了一个比较厌恶或者是说沉重，突然觉得这样子的生活过得有点没意思，具体用哪没有意思，呢说不上来。\n\n \n\n讲话人1 - 0:48\n\n现在心中想着的是那个 wait but why动画中地球对着那个火柴人说“走去过你想要的生活”。觉得现在的生活对我来说好像有点束缚，我不知道我这一天到底在做什么，我真不知道，是没有记录，没有进步还是怎么的？激情好像有点退却。\n\n \n\n讲话人1 - 1:25\n\n现在身旁的树有树叶，噢对了，我还看了车的视频，问界9五十万，有可能我心里的压力有一部分也是来自于那里，就是50万，呀唉我得赚多久才能赚到这部分钱？想暴富了，突然间也不是突然间吧。往前走，前面是一片黑暗。\n\n \n\n讲话人1 - 2:08\n\n就往前吧多走几步，也不会怎么办？我的右手小拇指呀它盘着了，我一伸直它就咔一声咔一声咔一声的响，那前方是什么路？\n\n \n\n讲话人1 - 2:38\n\n我过去我其实不是很懂，我现在就相当于拿着一个手表在这边自说自话在说什么，啊非常感谢小李今天给我打了第二通电话，他很关心我的情绪，让我感觉到我被人关心，想着昨天看了一些股票相关的消息，想着买些猪肉股，买些新能源，买些港股中概互联的ETF，想着也许能通过这些交易赚一些钱，赚钱来干嘛？\n\n \n\n讲话人1 - 3:44\n\n挣钱来让自己开心。钱不会让我开心，但是不纠结也许会让我开心一点，之前好想买个投影，现在呢现在不知道了，就像我之前好想买这个手表，好想买那个手机一样，当我拥有它了，它成了一种方式，没有当时我寄希望于它那样对我来说有那么大的帮助，但它也不是一无是处，比如这个手表在我手机快没电的时候，我能拿它来录个音，这大概也是极好的，但在平常他在我身边好像什么都没有作用，谁知道呢说到这里我每天的生活在干嘛？\n\n \n\n讲话人1 - 4:49\n\n呢我有点担心自己，没有反省自己的生活，自己的右手的小拇指有点疼，自己没办法减下肥，晚上一有点饥饿感就又想去吃东西，这应该是没有意志力吧。或者我也不知道是怎么回事，好不容易现在有个机会对着手表说一下这些话，AGI会不会替换掉我们的生活？半导体的那些公式太复杂了，电学呀一点都不会，会不了一点啊太难了。\n\n \n\n讲话人1 - 6:22\n\n前路其实有点长，我意识到自己有些普通，甚至于说自己有点平凡，嗯今天没有后面的豪言壮志了，就和这种情绪在一起待一会儿，吧它可能是个很短的一句话，把最后一句话送给自己。再会。\n\n> “去吧，去创造自己的生活”\n\n\n![去吧，去创造自己的生活](/images/492fcb9f479780f22c8baf499650a85.jpg)\n","tags":["个人"]},{"title":"LeetCode26","url":"/2020/11/23/LeetCode26/","content":"\n关键词：数组去重\n题目描述：\n\n> Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length.\n> Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.\n\n这道题需要更改原始输入的数组并且返回去重后的数组长度。\n分为两部分来思考：\n1. 去重数组长度计算\n注意到这里的输入数组是排好序的数组。所以可以用指针从头开始比较。并设置去重后的数组长度为count=0. 用count也可以作为数组的指针，来表示数组在count这个位置的数值。\n肯定要遍历整个数组，用一个for循环和i指针来做。\n在这里我一开始的思路是判断count指针和i指针大小，如果count指针所指的数小则将count++。最后count就是要求的数组长度。\n2. 更改原始输入数组\n题目要求必须在原始数组上进行更改而不能新建一个数组。有一个隐含前提，去重数组的长度一定小于等于原始数组。那么，我们可以直接将大于当前指针的数放入前一个已经去重的数之后。\n所以我们需要一个指针来保存去重列表的末尾部分,恰好这个指针就是count，当i指针遍历整个数组遇到一个新的数（只需要比count的大）时，可以将这个数放入count之后的格子里，直到i遍历到达了整个原始数组的尾部。\n\n需要考虑边界条件。当输入数组nums的长度为0和1时，这个算法是否能够给出0,1的返回值？不能。count=0的初始设置在nums.length=1时还是等于0.因为count==i==0，并不会进入自增的过程。可以考虑返回count+1，然后在算法顶端加一个判断条件。数组的长度是否为0，如果是0，直接返回0就好了。\n\nJava版本的代码如下：\n\n```java\npublic int removeDuplicates(int[] nums) {\n    if(nums.length == 0){\n        return 0;\n    }\n    int count = 0;\n    for(int i=1; i< nums.length; i++) {\n        if(nums[count] < nums[i]){\n            nums[count+1] = nums[i];\n            count++;\n        }\n    }\n    return count+1;\n}\n```","tags":["Leet Code"]},{"title":"学习Kaldi：中文Aishell项目（上）","url":"/2020/11/23/学习Kaldi：中文Aishell项目（上）/","content":"\n这篇文章是学习Kaldi的第二篇。对应SUSTech CS310课程的Lab6和Lab7。\n第一篇里探索了如何对toy language（仅包含两个单音素单词）进行语言模型的建模。至于训练和解码的部分，时间条件和理解能力暂时不允许去整理。\n本篇文章的主要目标是理解复杂的中文多音素语言模型和使用AiShell语料集来真实的训练出一个可用的中文语音识别模型。完整的AiShell例子包含GMM-HMM和神经网络。Lab6先展示了GMM-HMM后的结果。Lab7则补充了神经网络。\n\n## AiShell描述和下载\n\nAiShell 是 ？？公司开源的中文普通话语料集。400个来自不同方言区的人参与录制， 录制的条件是在室内使用高保真的麦克风，音频降采样到16000Hz。\n//中文文字脚本95%的准确度\n//170小时的语料。划分为85%的训练集，10%的开发集（作用？），5%的测试集。在上课的时候我被录制语料的成本吓到了，2000小时的语料大约需要100万人民币的费用。\nAiShell语料集可以免费由于学术目的。\n\n语料集下载\nKaldi中包含Aishell的示例脚本。在`kaldi/egs/aishell/s5`中。下文所有的文件都在该目录之下。\n下载语料集的脚本包含在`run.sh`中。\n先安装好语言模型的工具才能运行`run.sh`\n```\nrun ./install_kaldi_lm.sh && source ../env.sh\n```\n上一篇文章没有说每一个项目下的s5文件夹中有什么，在网上找到了别人写的一个总结：[kaldi 源码分析(三) - run.pl 分析](https://www.jianshu.com/p/6ab663601da8)\n```\ncmd.sh                     # 并行执行命令，通常分 run.pl, queue.pl 两种\nconfig                       # 参数定制化配置文件， mfcc, decode, cmvn 等配置文件\nlocal                         # 工程定制化内容\npath.sh                    # 环境变量相关脚本\nrun.sh                      # 整体流程控制脚本，主入口脚本\nsteps                       # 存放单一步骤执行的脚本\nutils                         # 存放解析文件，预处理等相\n关工具的脚本\n```\n最重要的入口脚本是run.sh。包含所有脚本。如果要在本地运行，需要修改这个脚本。把其中的`queue.pl`改成`run.pl`。\n```\nexport train_cmd=\"run.pl\" \nexport decode_cmd=run.pl \nexport mkgraph_cmd=\"run.pl\"\n```\n先做Lab6，注释掉神经网络训练部分。为了对比加不加神经网络对最后的识别准确率有多大的影响。\n```\n# nnet3\n#local/nnet3/run_tdnn.sh\n# chain\n#local/chain/run_tdnn.sh\n```\n\n\n\n## 运行run.sh脚本，一步到位\n在上一篇文章中，主要讲了kaldi的工作流程，复杂一点的项目除了要考虑多音素的对齐以外？基本流程是差不多的。先运行整体流程脚本run.sh看一下效果。然后再具体深入进脚本中看有哪些关键步骤。\n\n你是否遇到过连接远程服务器跑训练，然后网络掉线杀掉了正在跑的进程？我遇到过，后来主要使用nohup来避免这个问题。课件里推荐使用screen来避免远程登陆进程被杀掉后，训练进程也停止的问题。screen的原理不是本篇文章关心的重点。\n加上screen后运行run.sh：\n```\nscreen -S run\nrun ./run.sh\n```\n就能看到脚本在一个新的页面输出内容了。\n如果要结束进程` ctrl a + d`//我其实不喜欢这个命令，因为很经常使用ctrl+a来编辑命令，两个快捷键冲突。\n\n\n## 查看结果\n中文语音识别的准确度通常使用CER（Char Error Rate）来表示。因为中文中字是最小语义单位，而英文中词是基本语义单位。\n和上一篇文章差不多的命令。脚本的运行结果保存在了exp目录下。\n```\nfor x in exp/*/decode_test; do [ -d $x ] && grep WER $x/cer_* | utils/best_wer.sh; done 2>/dev/null\n```\n训练出来的结果如下：\n![训练结果](/images/1246.png)\n\n\n\n可以`cat RESULTS`，和官方跑出来的结果做一下对比。\n![RESULTS](/images/1247.png)\n需要注意的是，和上篇文章的实验不一样的是，输出的结果是多行的，因为执行了多次的实验，上面的脚本输出的是每次实验最好的结果。\n我自己跑出来的最好结果是tri5a的cer_14_0.5而RESULTS中的GMM-HMM模型中最好的结果是tri5a的cer_13_0.5。两者CER都是12.12。每次实验本身都有一定的随机性。结果有一些误差是没问题的。为了确认模型有被正确的训练，查看自己结果的`tri5a/decode_test/cer_13_0.5`的CER是12.18，恰好不是最优解而已。这里的13和14是lmwt（语言模型权重）。具体的可以看上一篇文章。\n![cer_13_0.5](/images/1248.png)\n\n## 细节\n使用命令`cat run.sh | grep \"#\"`将run.sh脚本中的环节注释提取打印出来。其中倒数2，4行是我们在一开始注释掉的。可以看到基本可以分为准备、训练和获取结果三个部分。\n![run.sh注释部分](/images/1249.png)\n\n### 准备\n1. 下载语料集\n需要注意的是aishell语料集有大概20GB的大小。意味着需要很长的时间才能下载下来。我是直接用服务器里提前下好的语料集。\n![aishell目录概览](/images/1250.png)\n```\nlocal/download_and_untar.sh $data $data_url data_aishell || exit 1;\n```\n这里的 `a || b`是一个逻辑符号，代表着如果a执行失败则执行b。这里要放一个小插曲。去年面试阿里云的实习项目时，面试官开头就问了如何知道上一条linux命令是否成功执行。我当时不知道，现在要知道了。就是看变量`$?`的值，如果为0代表成功执行。这里的`exit 1`终止当前进程并且将`$?`设置为1。表示不成功执行。\n`$data` 是aishell在本机的位置，既可以新建一个空目录来下载，也可以指定到已经下好的路径，aishell 分为`resource_aishell`和`data_aishell`两部分来下载，脚本会通过检查每一个部分下是否有`.complete`文件来判断当前部分是否下载完全。如果没有才会到指定网址下载。\n2. Lexicon Preparation\n```\nlocal/aishell_prepare_dict.sh $data/resource_aishell || exit 1;\n```\n这个脚本的功能主要是将`resource_aishell`下的`lexicon.txt`复制到`data/local/dict`中。并且提取出`nonsilence_phones.txt`、`optional_silence.txt   `、`silence_phones.txt`和`extra_questions.txt`。用到了很多awk和perl的脚本。没看懂。（要是看懂了，第一次assignment就不会搞砸了）\n那就要求自己看懂把。\n\n![lexicon.txt](/images/1251.png)\n\n\n![提取extra_questions.txt的代码](/images/1252.png)\n\n![extra_questions.txt部分内容](/images/1253.png)\n\n![提取nonsilence_phones.txt代码](/images/1254.png)\n每行代表一组相同的base phone,包含各种不同的重音或者声调。\n![nonsilence_phones.txt部分内容](/images/1255.png)\n\n3. Data preparation\n```\nlocal/aishell_data_prep.sh $data/data_aishell/wav $data/data_aishell/transcript || exit 1;\n```\n`$data/data_aishell/wav`目录下放着的是音频文件。其中有两级目录`speaker/filename.wav`。\n`$data/data_aishell/transcript`目录下放着的是`aishell_transcript_v0.8.txt`文字翻录。\n这个shell脚本的功能是将`$data/data_aishell/wav`下的 `train`,`test`,`dev`分别建立索引。并且建立Kaldi能够理解的语料格式。具体有些什么可以参考上一篇文章和下面这一段脚本。\n```\n# Transcriptions preparation\nfor dir in $train_dir $dev_dir $test_dir; do\n  echo Preparing $dir transcriptions\n  # 将当前集合目录下的wav的文件名提取出来作为utt_id\n  sed -e 's/\\.wav//' $dir/wav.flist | awk -F '/' '{print $NF}' > $dir/utt.list\n  # 根据目录结构建立utt2spk的关系\n  sed -e 's/\\.wav//' $dir/wav.flist | awk -F '/' '{i=NF-1;printf(\"%s %s\\n\",$NF,$i)}' > $dir/utt2spk_all\n  # 按列合并utt.list和wav.flist，达到对音频文件的映射。\n  paste -d' ' $dir/utt.list $dir/wav.flist > $dir/wav.scp_all\n  utils/filter_scp.pl -f 1 $dir/utt.list $aishell_text > $dir/transcripts.txt\n  awk '{print $1}' $dir/transcripts.txt > $dir/utt.list\n  utils/filter_scp.pl -f 1 $dir/utt.list $dir/utt2spk_all | sort -u > $dir/utt2spk\n  utils/filter_scp.pl -f 1 $dir/utt.list $dir/wav.scp_all | sort -u > $dir/wav.scp\n  sort -u $dir/transcripts.txt > $dir/text\n  utils/utt2spk_to_spk2utt.pl $dir/utt2spk > $dir/spk2utt\ndone\n```\n#### 4. Phone sets, questions, L compilation\n```\nutils/prepare_lang.sh --position-dependent-phones false data/local/dict \\\n    \"<SPOKEN_NOISE>\" data/local/lang data/lang || exit 1;\n```\n上面shell脚本的目的是创建L.fst，音素模型，其中fst是Finite State Transducers（有限状态转换器）的缩写。找到这篇[Kaldi学习笔记 -- 构建字典FST脚本 -- prepare_lang.sh 关键内容解析](https://blog.csdn.net/DuishengChen/article/details/52473918)详细的说明了这个脚本的工作。而[关于prepare_lang的一点理解](https://blog.csdn.net/mengjianmuzi/article/details/99499343)给脚本进行了一些翻译和注释。\n\n> [Kaldi中FST(Finite State Transducer)含义及其可视化](https://www.jianshu.com/p/4ad2add56b25)\n> L.fst: 音素词典（Phonetic Dictionary or Lexicon）模型，phone symbols作为输入，word symbols作为输出，如图Figure 1所示。\n> ![Figure 1 L.fst结构](/images/1256.png)\n> L_disambig.fst是为了消除模棱两可（disambiguation）而引入的模型，表述为 the lexicon with disambiguation symbols。分歧的情况如：一个词是另一个词的前缀，cat 和 cats在同一个词典中，则需要\"k ae t #1\"； 有同音的词，red: \"r eh d #1\", read: \"r eh d #2\"。\n\n我一直疑惑`lexiconp.txt`是怎么生成的，查了好久。结果竟然只是一段在词和音素之间插入1.0的代码：\n```\nif [[ ! -f $srcdir/lexiconp.txt ]]; then\n  echo \"**Creating $srcdir/lexiconp.txt from $srcdir/lexicon.txt\"\n  perl -ape 's/(\\S+\\s+)(.+)/${1}1.0\\t$2/;' < $srcdir/lexicon.txt > $srcdir/lexiconp.txt || exit 1;\nfi\n```\n![lexiconp.txt](/images/1257.png)\n那么L.fst是怎么得到的呢？\n通过`make_lexicon_fst.py`（现有的博客都说是.pl结尾，可能kaldi重构了）。还有一个消歧义的过程。具体的就看不懂了。[解码图创建示例（测试阶段）](https://shiweipku.gitbooks.io/chinese-doc-of-kaldi/content/decoding_graph_test.html)里有较为详细的文档讲解。\n\n#### 5. LM training\n```\nlocal/aishell_train_lms.sh || exit 1;\n```\n这个shell脚本读取`data/local/train/text`,`data/local/dict/lexicon.txt`\n得到text的计数文件`word.counts`并以`word.counts`为基础添加`lexicon.txt`中的字（除了SIL）出现的次数到`unigram.counts`中。我就没深入看下去了，期间用到的脚本文件有:`get_word_map.pl`、`train_lm.sh --arpa --lmtype 3gram-mincount $dir || exit 1`;这个步骤的结果保存在`data/local/lm/3gram-mincount/lm_unpruned.gz`中。\n#### 6. G compilation, check LG composition\n```\nutils/format_lm.sh data/lang data/local/lm/3gram-mincount/lm_unpruned.gz \\\n    data/local/dict/lexicon.txt data/lang_test || exit 1;\n```\n这个步骤是编译G.fst并将LG串联起来。\n> [Kaldi中FST(Finite State Transducer)含义及其可视化](https://www.jianshu.com/p/4ad2add56b25)\n> G.fst: 语言模型，大部分是FSA（finite state acceptor, i.e. 每个arc的输入输出是相同的），如图Figure 2所示。\n> ![Figure 2 G.fst结构（由指令词识别1-gram语法产生，disambiguation symbol #0 未加入）\n> ](/images/1258.png)\n\n>[kaldi 训练 aishell 解析](https://hupeng.me/articles/25.html)\n>utils/format_lm.sh:上述的语言工具基于第三方工具，为ARPA-format,脚本的作业是将其转换为fst，方便与之前的字典fst(L.fst)结合，发挥fst的优势。脚本最后会检测G.fst中是否存在没有单词的空回环，如果存在会报错，因为这会导致后续HLG determinization的出现错误。\n>脚本utils/format_lm.sh解决把ARPA格式的语言模型转换成OpenFST格式类型。脚本用法如下：\n>Usage: utils/format_lm.sh <lang_dir> <arpa-LM> <lexicon> <out_dir>\n>E.g.: utils/format_lm.sh data/lang data/local/lm/foo.kn.gz data/local/dict/lexicon.txt data/lang_test\n>Convert ARPA-format language models to FSTs.\n>这个脚本的一些关键命令如下：\n>Kaldi程序arpa2fst将ARPA格式的语言模型转换成一个加权有限状态转移机（实际上是接收机）。\n\n流程很复杂，未来可能再把L.fst，LM training，G.fst， LG composition另起一篇。（就是说现在时间条件不允许深入）\n### 训练\n训练的环节开始我就读不懂了。主要是逻辑和概念不懂。也不浪费时间了。简单的去了解一下输入输出和功能。\n#### 1. MFCC 特征生成\n这个环节和yesno项目的没有不同。主要就是获得train,test, dev三个集合的归一化的梅尔倒谱系数。最后修复排序错误，并会移除那些被指明需要特征数据或标注，但是却找不到被需要的数据的那些发音（utterances）。\n#### 2. 训练单音素模型\n```\nsteps/train_mono.sh --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/mono || exit 1;\n```\n参考[kaldi-GMM-HMM pipeline](https://zhuanlan.zhihu.com/p/82380716)，上面的shell脚本主要是对齐音素和每一帧音频的。[Kaldi 入门train_mono.sh详解](https://blog.csdn.net/DanyHgc/article/details/75247158)、[kaldi学习笔记 -- 训练单音素（monophone）模型脚本 -- steps/train_mono.sh](https://blog.csdn.net/DuishengChen/article/details/52575926)都有讲一些。\n对流程讲得最好的是：\nmkgraph 需要 lang_test 下的 L.fst G.fst phones.txt, words.txt , phones/silence.csl , phones/[http://disambig.int](https://link.zhihu.com/?target=http%3A//disambig.int)\n\n以及 exp/tri 下的 tree, final.mdl\n\n> 在训练的job并行训练过程中，训练数据的各个子集合是分散到不同的处理器去进行训练，然后每轮迭代后会进行合并。\n> 下面就讲一下训练的过程：\n> 1.首先是初始化GMM，使用的脚本是/kaldi-trunk/src/gmmbin/gmm-init-mono，输出是0.mdl和tree文件；\n> 2.compile training graphs,使用的脚本是/kaldi-trunk/source/bin/compile-training-graphs，输入是tree,0.mdl和L.fst,输出是fits.JOB.gz，其是在训练过程中构建graph的过程；\n> 3.接下来是一个对齐的操作，kaldi-trunk/source/bin/align-equal-compiled；\n> 4.然后是基于GMM的声学模型进行最大似然估计得过程，脚本为/kaldi-trunk/src/gmmbin/gmm-est；\n> 5.然后进行迭代循环中进行操作，如果本步骤到了对齐的步骤，则调用脚本kaldi-kaldi/src/gmmbin/gmm-align-compiled；\n> 6.重新估计GMM，累计状态，用脚本/kaldi-trunk/src/gmmbin/gmm-acc-states-ali；调用新生成的参数(高斯数)重新估计GMM，调用脚本/kaldi-trunk/src/gmmbin/gmm-est；\n> 7.对分散在不同处理器上的结果进行合并，生成.mdl结果，调用脚本gmm-acc-sum；\n> 8.增加高斯数，如果没有超过设定的迭代次数，则跳转到步骤5重新进行训练\n> 最后生成的.mdl即为声学模型文件\n> 在离线识别阶段，即可以调用utils/mkgraph.sh；来对刚刚生成的声学文件进行构图\n> 之后解码，得到离线测试的识别率。\n\n#### 3. (Monophone decoding) 单音素解码\n```\nutils/mkgraph.sh data/lang_test exp/mono exp/mono/graph || exit 1;\nsteps/decode.sh --cmd \"$decode_cmd\" --config conf/decode.config --nj 10 \\\n  exp/mono/graph data/dev exp/mono/decode_dev\nsteps/decode.sh --cmd \"$decode_cmd\" --config conf/decode.config --nj 10 \\\n  exp/mono/graph data/test exp/mono/decode_test\n```\n`mkgraph.sh`将L_disambig.fst 和 G.fst 复合生成LG.fst。中间经历了我看不懂的处理。最终生成用于解码的 HCLG.fst。\n\n#### 看不懂的部分\n后面就已经看不懂了。\n```\n# Get alignments from monophone system.\nsteps/align_si.sh --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/mono exp/mono_ali || exit 1;\n\n# train tri1 [first triphone pass]\nsteps/train_deltas.sh --cmd \"$train_cmd\" \\\n 2500 20000 data/train data/lang exp/mono_ali exp/tri1 || exit 1;\n\n# decode tri1\nutils/mkgraph.sh data/lang_test exp/tri1 exp/tri1/graph || exit 1;\nsteps/decode.sh --cmd \"$decode_cmd\" --config conf/decode.config --nj 10 \\\n  exp/tri1/graph data/dev exp/tri1/decode_dev\nsteps/decode.sh --cmd \"$decode_cmd\" --config conf/decode.config --nj 10 \\\n  exp/tri1/graph data/test exp/tri1/decode_test\n\n# align tri1\nsteps/align_si.sh --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/tri1 exp/tri1_ali || exit 1;\n\n# train tri2 [delta+delta-deltas]\nsteps/train_deltas.sh --cmd \"$train_cmd\" \\\n 2500 20000 data/train data/lang exp/tri1_ali exp/tri2 || exit 1;\n\n# decode tri2\nutils/mkgraph.sh data/lang_test exp/tri2 exp/tri2/graph\nsteps/decode.sh --cmd \"$decode_cmd\" --config conf/decode.config --nj 10 \\\n  exp/tri2/graph data/dev exp/tri2/decode_dev\nsteps/decode.sh --cmd \"$decode_cmd\" --config conf/decode.config --nj 10 \\\n  exp/tri2/graph data/test exp/tri2/decode_test\n\n# train and decode tri2b [LDA+MLLT]\nsteps/align_si.sh --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/tri2 exp/tri2_ali || exit 1;\n\n# Train tri3a, which is LDA+MLLT,\nsteps/train_lda_mllt.sh --cmd \"$train_cmd\" \\\n 2500 20000 data/train data/lang exp/tri2_ali exp/tri3a || exit 1;\n\nutils/mkgraph.sh data/lang_test exp/tri3a exp/tri3a/graph || exit 1;\nsteps/decode.sh --cmd \"$decode_cmd\" --nj 10 --config conf/decode.config \\\n  exp/tri3a/graph data/dev exp/tri3a/decode_dev\nsteps/decode.sh --cmd \"$decode_cmd\" --nj 10 --config conf/decode.config \\\n  exp/tri3a/graph data/test exp/tri3a/decode_test\n\n# From now, we start building a more serious system (with SAT), and we'll\n# do the alignment with fMLLR.\n\nsteps/align_fmllr.sh --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/tri3a exp/tri3a_ali || exit 1;\n\nsteps/train_sat.sh --cmd \"$train_cmd\" \\\n  2500 20000 data/train data/lang exp/tri3a_ali exp/tri4a || exit 1;\n\nutils/mkgraph.sh data/lang_test exp/tri4a exp/tri4a/graph\nsteps/decode_fmllr.sh --cmd \"$decode_cmd\" --nj 10 --config conf/decode.config \\\n  exp/tri4a/graph data/dev exp/tri4a/decode_dev\nsteps/decode_fmllr.sh --cmd \"$decode_cmd\" --nj 10 --config conf/decode.config \\\n  exp/tri4a/graph data/test exp/tri4a/decode_test\n\nsteps/align_fmllr.sh  --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/tri4a exp/tri4a_ali\n\n# Building a larger SAT system.\n\nsteps/train_sat.sh --cmd \"$train_cmd\" \\\n  3500 100000 data/train data/lang exp/tri4a_ali exp/tri5a || exit 1;\n\nutils/mkgraph.sh data/lang_test exp/tri5a exp/tri5a/graph || exit 1;\nsteps/decode_fmllr.sh --cmd \"$decode_cmd\" --nj 10 --config conf/decode.config \\\n   exp/tri5a/graph data/dev exp/tri5a/decode_dev || exit 1;\nsteps/decode_fmllr.sh --cmd \"$decode_cmd\" --nj 10 --config conf/decode.config \\\n   exp/tri5a/graph data/test exp/tri5a/decode_test || exit 1;\n\nsteps/align_fmllr.sh --cmd \"$train_cmd\" --nj 10 \\\n  data/train data/lang exp/tri5a exp/tri5a_ali || exit 1;\n\n# nnet3\nlocal/nnet3/run_tdnn.sh\n\n# chain\nlocal/chain/run_tdnn.sh\n```\n\n### 获取结果\n```\n# getting results (see RESULTS file)\nfor x in exp/*/decode_test; do [ -d $x ] && grep WER $x/cer_* | utils/best_wer.sh; done 2>/dev/null\nfor x in exp/*/*/decode_test; do [ -d $x ] && grep WER $x/cer_* | utils/best_wer.sh; done 2>/dev/null\nexit 0;\n```\n和上一篇文章一样的步骤。\n[Kaldi入门：yesno项目](https://www.jianshu.com/p/09deba57f339)","tags":["Kaldi","ASR"]},{"title":"使用Python开发桌面应用的一个体验","url":"/2020/11/23/使用Python开发桌面应用的一个体验/","content":"\n我的主力编程语言现在是Python，平时也会使用Python写一些小脚本做文件处理（更简单一些的操作会直接用Shell命令）。基本不会接触到GUI界面的编写。但是呢，对于我而言命令行是可行的方案，但是如果要把代码交给没有编程基础的人，并且没有相关的开发环境时。就需要GUI和打包技术了。最近就遇到一个情况需要开发一个类似图片直方图均衡的功能给和课题组合作的医生使用，工作的电脑是离线的，医生没有编程基础。\n\n要求条件：\n\n1.  使用Python，快捷开发；\n\n2.  GUI；\n\n3.  独立的EXE可执行文件。\n\n技术栈：\n\n1.  Numpy，用于对像素数组进行映射；\n\n2.  Pillow，图像库，底层调用numpy；\n\n3.  TkinterDND2使用Python内置的Tkinter的改进版，支持基础的code based UI和文件拖拽功能。使用conda安装。\n\n我的开发环境是OS X，幸好之前一个老师分配的Windows虚拟机还能用。Python环境3.7.3。\n\n## 第一版\n\n![image.png](/images/1240.png) \n\n支持选取图片文件夹，递归查找其中的jpg文件并处理。\n\n### 文件夹选取功能\n\n### **![image.png](/images/1241.png)**\n\n### 图片处理函数\n\n### **![image.png](/images/1242.png)**\n\n简单的对图片像素映射到0-255的范围。能够使得标本图片更容易标注。\n\n### 恢复函数\n\n### **![image.png](/images/1243.png)**\n\n相当于图片处理函数的逆操作，在图片处理的时候保存了每一张图片的处理参数。\n\n### 性能分析：\n\n### **![image.png](/images/1244.png)**\n\n因为一个标本采集的图片有接近2000张，测试时预估大约需要8分钟才能完成一个标本的图片优化。而费\n\n使用cProfile进行性能分析，61张图片要18秒多。平均一秒3张图片。主要的时间消耗是在转换成numpy矩阵、保存图片和图片处理函数。基本是不能够再优化的。那么就要从数据量方面考虑，是否每一张图片都需要优化并标注？其实不是的，每一个标本只需要选取最多10张进行标注，那么由医生选取并且拖放到指定界面会更快捷。\n\n## 第二版\n\n## **![image.png](/images/1245.png)**\n\n第二版添加了拖放功能并且把文件夹选取的功能去掉了。可以拖放多个文件，并且会筛选处理其中的jpg文件。\n\nPS: 连标题都懒得改。。\n\n## pyinstaller打包\n\n使用pip 安装pyinstaller打包，pyinstaller是系统依赖的，在什么系统打包，在什么系统使用。坑很多。\n\n## 总结\n\n使用pyinstaller遇到几个bug。six，warm\n\n打包太大了。有500MB。代码量180行。\n\n加了Frame之后文件夹输入框就不更新了。可能的原因是pack和grid在frame和widge之间混用。或者混用Tkinter和TkinterDnD2。因为文件夹选取的方案被替换掉了。所以这个bug不再存在。\n\nconda环境安装耗时较久。\n\n还是java香。","tags":["Python"]},{"title":"以Notion为基础的自我管理体系","url":"/2020/11/23/以Notion为基础的自我管理体系/","content":"\n我常用[Todoist](https://www.notion.so/imchenmin/Todoist-7dc5a0cd012e41a6b2cfef696e869a00) 管理自己的任务，用[印象笔记](https://www.notion.so/imchenmin/175adc9e20194d2b97062999b44255bb)存放感兴趣的文本和自己觉得需要保存的文本。\n\n后来因为Notion的数据库特质吸引到我。所以整合之前的项目管理、任务管理和笔记文本管理在Notion之中。\n\nNotion的缺点在于过于自由，需要加一个框架来使之符合项目管理、任务管理、文本笔记管理的功能。\n\n使用前后端分离的方式，将整个Notion空间分为View和Model，用一些自定义的数据库来组织不同类型的数据，位于根目录Resources之下。\n\n## 数据库\n\nResource\n\n*   Notes（笔记）\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-021.01.51.png](/images/1260.png) \n\n*   Book（书籍）\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-025.05.59.png](/images/1261.png) \n\n*   Video（想看的，看过的视频）\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-025.08.20.png](/images/1262.png) \n\n*   People（人脉）\n\n*   Thing（事情，分为Task和Calendar）\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-025.09.09.png](/images/1263.png) \n\n*   Tag（标签）\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-026.28.14.png](/images/1264.png) \n\n*   Subscription（订阅服务）\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-026.29.41.png](/images/1265.png) \n\n*   Project（基于项目的自我管理），与Things、note联系。\n\n    ![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-029.09.57.png](/images/1266.png) \n\n## Dashboard\n\n主要通过Dashboard这一个页面来管理自己当天的任务、任务收件箱和笔记。\n\n![Notion%2059beed18f91a433abd3b3c79042c9eb7/2020-07-026.35.02.png](/images/1267.png) \n\n## 善用过滤器和视图类型\n\n数据库可以自选多种视图类型，如列表，日历，卡片，表格等。并且可以使用过滤功能。但是一个视图只能有一个过滤器。通过复制视图可以达到快速创建相同类型的视图和不同的过滤功能。\n\n## 适用情况与不适用情况\n\nNotion到目前为止都不能避免的两个大山：\n\n1.  GFW\n\n2.  Web App\n\n第一点导致很难让其他人访问，导致作为团队共享或者公开访问需要较大的门槛。\n\nNotion到现在为止都是以HTML，CSS，JS为基础的。所谓的客户端都是网页，并没有多强的离线功能，更别提下载某一个页面。与系统的整合很差。没有单独的页面快捷入口，比起Todoist和印象笔记存在较大的稳定问题。\n\n如果网络环境不好，要做好不能够随时访问重要信息的准备！\n\n所以我会有一个Tag叫Save to Evernote来手动将重要的信息保存到印象笔记并且离线下来，遇到任务也会先放到Todoist中，等能连上网的时候再同步。\n\n原文链接：https://www.notion.so/imchenmin/Notion-d7b4ef1b46aa4ef7b73dc3fb25d5a2d1","tags":["个人","知识管理"]},{"title":"Ubuntu 18.04静态IP设置和遇到的虚拟机网卡问题","url":"/2020/11/23/Ubuntu-18-04静态IP设置和遇到的虚拟机网卡问题/","content":"\n帮同学的实验室配置静态IP，将其中的流程和关键记录下来。\n\n校园网内使用DHCP来分配IP地址，有效期为2天，如果两天之内设备未续租，IP地址会被收回。\n\n实验室的服务器需要远程登陆，需要一个静态IP来保证服务器的可访问性。\n\n在申请了几个静态IP之后，会获得一组数据。1) 172.xxx.xxx.xxx、2) 255.255.255.0、3) 172.xxx.xxx.254、4) vlanxx。其中1）是静态IP的地址，2）是子网掩码，也可以用24表示。3）网关地址，4）交换机的vlan接口，由交换机管理员配置。\n\n学校中由网络中心运营管理交换机，需要先联系工程师在网关中添加vlanxx接口，并且把服务器的所有MAC地址绑定静态IP地址组。\n\n服务器的操作系统和版本是Ubuntu 18.04 server。网络管理使用netplan。之前同学装了gnome-shell。我先尝试使用简易桌面自带的网络管理程序，ifconfig -a发现对网卡配置不起作用。\n\n直接修改netplan程序位于\\etc\\netplan\\01-netcfg.yaml。\n\n当时该文件基本的结构如下：\n```\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    eno1:\n      dhcp4: true\n  bridges:\n    vir0br:\n      interfaces: [eno1]\n      dhcp4: true\n```\n需要注意的就是eno1和vir0br。其中dhcp4代表着是否使用ipv4版本的dhcp地址分配。\n\nvir0br是[https://virt-manager.org/](https://virt-manager.org/)虚拟机管理器NAT模式下的网桥接口。服务器中通过virt-manager运行着windows 系统。\n\n配置静态ip需要修改eno1的配置。\n```\neno1:\n  dhcp4: no\n  addresses: [172.xxx.xxx.xxx/24]\n  gateway4: 172.xxx.xxx.254\n  nameservers:\n    addresses: [8.8.8.8, 8.8.4.4]\n```\n然后netplan apply。接着ifconfig -a 查看网卡ip是否更改，如果没有更改，不用着急。很可能是配置文件的问题。而不是需要重启或者sudo service networking restart重启网卡。\n\n需要注意的是这里有一个bridge。bridge是一个虚拟网络设备，具有网络设备的特征，可以配置IP，MAC地址等。bridge不分接入进来的设备是虚拟的还是物理的，当eno1加入vir0br之后，从外面网络收到的数据包将无条件的转发给vir0br，自己变成一根网线。先用brctl show查看网桥状态然后.ifconfig <网桥名> down, brctl delbr <网桥名>删除网桥，将01-netcf.yaml中的bridge 部分删掉。（但是会造成虚拟机网络异常）。 重新netplan apply，完成静态IP的配置。\n\n对于windows虚拟机的网络配置还在检索学习中。网上较少Linux寄主机，Windows虚拟机的配置情况。","tags":["计算机网络","Linux"]},{"title":"OpenCV和numpy笔记","url":"/2020/11/22/OpenCV和numpy笔记/","content":"\n使用K-means做视频图像分割的作业的时候，遇到一个问题值得记录。\n\n该代码的功能是使用K-means做图像分割并保存图像与视频。\n\n遇到的问题是kmean之后得到的是每一个像素点的分类标签，需要将这些标签可视化成为分割(Segmentation)的结果视频。一开始没有直接找到相关的函数，搜索查询后给的建议都是使用`matlibplot`的`plt.imshow()`函数来实现，但是我不想要每次保存成为图片再转成视频，考虑到opencv-python的底层使用`numpy.array`来表述图像的。一个BGR图像相当于一个3维uint8数组（w,h,c)。要做的就是生成一个标签到像素的映射关系。这里我用的是`matplotlib.cm` color map 功能。\n\n```\nx = np.linspace(0.0, 1.0, 10)\nrgb = cm.get_cmap(plt.get_cmap('Set1'))(x)[np.newaxis, :, :3][0]\n```\n\n\n\n\n\n```\nimport numpy as np\nfrom numpy.matlib import repmat\nfrom sklearn.preprocessing import normalize\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom sklearn.cluster import KMeans\nimport cv2\nfrom PIL import Image\n\nn_cl=5\n\nvideoCapture = cv2.VideoCapture('road_video.mov')\nfps = videoCapture.get(cv2.CAP_PROP_FPS)\nsize = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\nvideoWriter = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), (fps/10), size)\n\ndef kmeans(data, n_cl, verbose=True):\n    n_samples, dim = data.shape\n    centers = data[np.random.choice(range(n_samples), size=n_cl)]\n    old_labels = np.zeros(shape=n_cl)\n    while True:\n        distances = np.zeros((n_samples, n_cl))\n        for cluster_idx, cluster in enumerate(centers):\n            distances[:, cluster_idx] = np.sum(np.square(data - repmat(cluster, n_samples, 1)), axis=1)\n        new_labels = np.argmin(distances, axis=1)\n        for l in range(0, n_cl):\n            centers[l] = np.mean(data[new_labels==l], axis=0)\n        if verbose:\n            fig, ax = plt.subplots()\n            ax.scatter(data[:, 0], data[:, 1], cluster=new_labels, s=40)\n            ax.plot(centers[:, 0], centers[:, 1], 'r*', markersize=20)\n            plt.waitforbuttonpress()\n            plt.close()\n        if np.array_equal(new_labels, old_labels):\n            break\n        old_labels = new_labels\n    return new_labels\n\ncount = 0\nx = np.linspace(0.0, 1.0, 10)\nrgb = cm.get_cmap(plt.get_cmap('Set1'))(x)[np.newaxis, :, :3][0]\n\nwhile True:\n    print(count)\n    count += 1\n#   load the frame\n    success, frame = videoCapture.read()\n    if not success:\n        break\n    img = np.float32(frame)\n    h,w,c = img.shape\n    # print(h,w,c)\n#   add coordinates\n    row_indexes = np.arange(0, h)\n    col_indexes = np.arange(0, w)\n    coordinates = np.zeros(shape=(h,w,2))\n    coordinates[..., 0] = normalize(repmat(row_indexes, w, 1).T)\n    coordinates[..., 1] = normalize(repmat(col_indexes, h, 1))\n    data = np.concatenate((img, coordinates), axis=-1)\n    data = np.reshape(data, newshape=(w *h, 5))\n    labels = kmeans(data, n_cl=n_cl, verbose=False)\n\n    print('after')\n    labels = labels.flatten()\n    segmented_image = rgb[labels.flatten()]\n    # print(segmented_image)\n    segmented_image = segmented_image.reshape(img.shape)\n    plt.imshow(segmented_image)\n    # # plt.imshow(np.reshape(labels, (h, w)), cmap=\"hsv\")\n    plt.savefig(\"lab9/\"+str(count-1))\n    segmented_image = segmented_image *255\n    segmented_image = segmented_image.astype('uint8')\n    videoWriter.write(segmented_image)\n\nvideoWriter.release()\nvideoCapture.release()\n```\n\n## 参考网页\n\nhttps://blog.csdn.net/llh_1178/article/details/77833447\n\nhttps://docs.opencv.org/3.4/d4/dba/classcv_1_1viz_1_1Color.html\n\nhttps://realpython.com/python-opencv-color-spaces/\n\nhttps://www.thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\n\nhttps://pvss.github.io/Opencv+Python.html\n\nhttps://stackoverflow.com/questions/9280653/writing-numpy-arrays-using-cv2-videowriter\n\n","tags":["OpenCV","Numpy","Python","编程记录"]},{"title":"新服务器迁移postgres数据库","url":"/2020/11/21/新服务器迁移postgres数据库/","content":"\n课题组要部署一个云服务，之前用的是我个人申请的学生版服务器，价格优惠。快到期需要续费，但是优惠版的服务器不能够继续以优惠价格续费，正常的价格是一千多每年。所以我决定新申请一个优惠版服务器，然后迁移Postgres数据库和部署的web server。\n\n基本步骤是购买新服务器（Ubuntu18.04版本），密码重设，为了方便登录也可以设置ssh免密码登录。\n\n## 安装配置Postgres\n\n我觉得Postgres比MySQL好的一点在于其开源。且能实现数据库的基本功能。\n\n### 服务器安装Postgres\n\n`sudo apt-get install postgresql postgresql-client `\n\n可以在本机安装一个postgres-client，免得在两个服务器之间来回登录。\n\n### 配置postgres数据库账号和远程连接\n\n登录新服务器，设置linux中的postgres用户密码\n\n`sudo passwd postgres`或者`sudo -i -u postgres`，免密码登录。\n\n设置postgres 中postgres用户密码(以postgres用户登录)\n\n`psql`进入数据库clinet软件\n\n`postgres \\password`设置数据库管理员postgres的密码\n\n### 远程连接设置\n\n修改Postgres远程连接允许 \n\n`sudo vim /etc/postgres/10/main/postgres.conf`， \n\n修改`listen_addresses`那一行为`listen_addresses = '*'`（有的公司要求不允许数据库开放外网访问，请谨慎该选项）。\n\n修改远程登录选项\n\n`vim  /etc/postgres/10/main/pg_dba.conf`\n\n添加新行\n\n` host  all  all 0.0.0.0/0 md5` (不要使用trust，除非你想任何人能够访问你的数据库内容)\n\n重启服务以应用刚刚的修改\n\n`sudo service postgresql restart`\n\n[参考](http://lazybios.com/2016/11/how-to-make-postgreSQL-can-be-accessed-from-remote-client/)\n\n## 数据库迁移\n\nPostgres 提供导出数据库数据和结构的程序。有两种方法，一种是在本机安装psql程序，新旧服务器都进行上面的远程连接设置，然后本机连接源服务器数据库dump备份文件，然后本机连接目标服务器数据库导入数据；另外一种是登录源服务器dump备份文件，再导出到目标服务器，在目标服务器执行导入操作。我使用的是比较麻烦的第二种。\n\n### 备份数据库\n\n`pg_dump -h (ip or localhost) -U postgres databasename > databasename.bak`\n\n我是在服务器本地进行的操作，所以可以使用localhost。\n\n### 恢复数据库\n\n恢复数据库指令不会创建新的数据库，所以需要先行在新的数据库中创建数据库 `create database databasename;`\n\n然后执行`psql -h localhost -U postgres -d databasename < databasename.bak`恢复数据库。\n\n[参考](https://juejin.cn/post/6844904002409201671)","tags":["Linux","运维","Postgres"]},{"title":"Kaldi入门(一):yesno项目","url":"/2020/05/01/Kaldi入门-一-yesno项目/","content":"\n这个学期选了一门自然语言处理课，结果这门课主要的研究课题是自动语音识别（ASR）。既然入了这个坑。就先好好了解一下如何做ASR吧。\n老师Tom Ko要求使用Kaldi这个工具来做ASR。课上到一半才知道Kaldi中有几千行的脚本代码是老师提交的。好吧，脚本好难的。\n为了入门Kaldi，课程的第5次Lab是一个mini projec: yesno\n\n首先要下载并编译Kaldi，安装的过程不是我的学习重点，可以先参考[Kaldi的下载安装与编译]([https://blog.csdn.net/snowdroptulip/article/details/78896915](https://blog.csdn.net/snowdroptulip/article/details/78896915)\n)，在漫长的编译过程之后假设已经安装好了Kaldi。\n\n## 项目目录结构\nyesno项目的脚本和README都在`kaldi/egs/yesno`之下。\nREADME.txt文件中包含数据集描述：\n```shell\nThe \"yesno\" corpus is a very small dataset of recordings of one individual\nsaying yes or no multiple times per recording, in Hebrew.  It is available from\nhttp://www.openslr.org/1.\nIt is mainly included here as an easy way to test out the Kaldi scripts.\n\nThe test set is perfectly recognized at the monophone stage, so the dataset is\nnot exactly challenging.\n\nThe scripts are in s5/.\n```\n数据脚本路径： kaldi/egs/yesno/s5。在下面执行的很多操作都可以直接调用已经写好的脚本来执行，之所以深入到具体的流程中是为了加强对ASR流程的理解。\n\n## 下载数据集\n第一步是从网络上下载数据集文件`waves_yesno.tar.gz`到s5/路径下并解压。\n原始的数据是60个.wav文件。文件名是八个用下划线分隔的01组合。需要将音频数据转化成kaldi能够处理的格式。\n\n\n## 转换成Kaldi能处理的格式\n下载完数据集后，将数据集划分为31个训练，30个测试（数量大致相当）。在s5/下创建data文件夹，把划分好的音频文件放入train_yesno和test_yesno。\n\nKaldi使用以下几个文件来表示数据：\n1. Text\n音频的文本记录。每一个音频文件一行。格式为`<utt_id> <transcript>`。<utt_id>为音频的id，一般用不带扩展名的文件名表示。utt_id在wav.scp文件中与具体的文件映射。<transcript>是音频对应的文字。\n2. wav.scp\n将文件映射到唯一的utt_id。\n格式为`<utt_id> <path or command to get wave file>`\n第二个参数既可以是对应utt_id的音频文件路径，也可以是能够获得音频文件的指令。\n3. utt2spk\n对于每一个音频文件，标记是哪一个人发音的。因为yesno数据集中只有一个发音者，用global来表示所有的utt_id\n文件内每一行的格式为`<utt_id> <speaker_id>`\n\n4. spk2utt\n和3反过来。文件内每一行对应一个发音者，第一个是speaker的id，后面用空格分隔开60个utt_id。格式为`<speaker_id> <utt_id1> <utt_id2> ...`\n\n本步骤可直接调用脚本：\n```bash\ncd kaldi/egs/yesno/s5\nlocal/prepare_data.sh waves_yesno\n```\n读了一下prepare_data.sh的脚本\n```bash\n#!/usr/bin/env bash\n\nmkdir -p data/local\nlocal=`pwd`/local\nscripts=`pwd`/scripts\n\nexport PATH=$PATH:`pwd`/../../../tools/irstlm/bin\n\necho \"Preparing train and test data\"\n\ntrain_base_name=train_yesno\ntest_base_name=test_yesno\nwaves_dir=$1\n\nls -1 $waves_dir > data/local/waves_all.list\n\ncd data/local\n\n../../local/create_yesno_waves_test_train.pl waves_all.list waves.test waves.train\n\n../../local/create_yesno_wav_scp.pl ${waves_dir} waves.test > ${test_base_name}_wav.scp\n\n../../local/create_yesno_wav_scp.pl ${waves_dir} waves.train > ${train_base_name}_wav.scp\n\n../../local/create_yesno_txt.pl waves.test > ${test_base_name}.txt\n\n../../local/create_yesno_txt.pl waves.train > ${train_base_name}.txt\n\ncp ../../input/task.arpabo lm_tg.arpa\n\ncd ../..\n\n# This stage was copied from WSJ example\nfor x in train_yesno test_yesno; do\n  mkdir -p data/$x\n  cp data/local/${x}_wav.scp data/$x/wav.scp\n  cp data/local/$x.txt data/$x/text\n  cat data/$x/text | awk '{printf(\"%s global\\n\", $1);}' > data/$x/utt2spk\n  utils/utt2spk_to_spk2utt.pl <data/$x/utt2spk >data/$x/spk2utt\ndone\n```\n## 建立词典\n\n\n对于当前项目，我们只有两个词 Ken(yes) 和 Lo（no)。但是在真实的语言中，词的数量不可能这么少，并且还有停顿和环境噪声。kaldi将这些非语言的声音称作slience（SIL）。\n加上SIL一共需要三个词来表示当前这个yesno语言模型。\n\n调用脚本：\n```bash\nlocal/prepare_dict.sh\n```\n将会在s5/data/local/dict中看到新生成的5个文件。\n1. lexicon.txt\n```\n<SIL> SIL\nYES Y\nNO N\n```\n2. lexicon_words.txt\n比1少第一行\n3. nonsilence_phones.txt\n```\nY\nN\n```\n4. silence_phones.txt\n```\nSIL\n```\n5. optional_silence.txt\n和4一样\n\n这个脚本本身只是将input文件夹下面的lexicon_nosil.txt，lexicon.txt， phones.txt复制到dict所在目录，并且加上SIL。\n\n\n## 语言模型\n\n接下来要做语言模型。\n项目提供了一个一元的语言模型。然而我们需要将这个模型转换成一个WFST（一种有穷自动机）\n执行:\n ```bash\nutils/prepare_lang.sh --position-dependent-phones false data/local/dict/ \"<SIL>\" data/local/lang/ data/lang\nlocal/prepare_lm.sh\n ```\n\nprepare_lang.sh的开头注释如下：\n```\n# This script prepares a directory such as data/lang/, in the standard format,\n# given a source directory containing a dictionary lexicon.txt in a form like:\n# word phone1 phone2 ... phoneN\n# per line (alternate prons would be separate lines), or a dictionary with probabilities\n# called lexiconp.txt in a form:\n# word pron-prob phone1 phone2 ... phoneN\n# (with 0.0 < pron-prob <= 1.0); note: if lexiconp.txt exists, we use it even if\n# lexicon.txt exists.\n# and also files silence_phones.txt, nonsilence_phones.txt, optional_silence.txt\n# and extra_questions.txt\n# Here, silence_phones.txt and nonsilence_phones.txt are lists of silence and\n# non-silence phones respectively (where silence includes various kinds of\n# noise, laugh, cough, filled pauses etc., and nonsilence phones includes the\n# \"real\" phones.)\n# In each line of those files is a list of phones, and the phones on each line\n# are assumed to correspond to the same \"base phone\", i.e. they will be\n# different stress or tone variations of the same basic phone.\n# The file \"optional_silence.txt\" contains just a single phone (typically SIL)\n# which is used for optional silence in the lexicon.\n# extra_questions.txt might be empty; typically will consist of lists of phones,\n# all members of each list with the same stress or tone; and also possibly a\n# list for the silence phones.  This will augment the automatically generated\n# questions (note: the automatically generated ones will treat all the\n# stress/tone versions of a phone the same, so will not \"get to ask\" about\n# stress or tone).\n```\n通过阅读脚本和脚本中的注释。可以知道`prepare_lang.sh`的用法\n```\nUsage: utils/prepare_lang.sh <dict-src-dir> <oov-dict-entry> <tmp-dir> <lang-dir>\ne.g.: utils/prepare_lang.sh data/local/dict <SPOKEN_NOISE> data/local/lang data/lang\n--position-dependent-phones (true|false)        # default: true; if true, use _B, _E, _S & _I\n```\n<dict-src-dir>是我们在上一部分生成的词典目录。需要包含lexico.txt，extra_questions.txt，nonsilence_phones.txt，optional_silence.txt  silence_phones.txt。\n`position_dependent_phones`参数为false，导致解码后不能算出单词边界。很多的脚本，特别是评分脚本将不能正常运行。\n第二个指令将语言模型转换成G.fst格式并保存在data/lang_test_tg 目录下。\n这个脚本的核心内容是调用了arpa2fst和fstisstochastic，再创建了G.fst之后检查是否有空字符（<s>,</s>之类的）的循环。\n```\n  arpa2fst --disambig-symbol=#0 --read-symbol-table=$test/words.txt input/task.arpabo $test/G.fst\n\n  fstisstochastic $test/G.fst\n```\n[arpa2fst 原理详解](https://blog.csdn.net/yutianzuijin/article/details/78756130)\n> arpa文件可以很容易地表示任意n-gram语言模型，不过在实际中n通常等于3、4或者5。arpa文件的每一行表示一个文法项，它通常包含三部分内容：probability word(s) [backoff probability]。probability表示该词或词组发生的概率，word(s)表示具体的词或者词组。backoff probablitiy是可选项，表示回退概率。\n\n在yesno这个toy project中只使用1元的语言模型。对应的arpa文件在`input/task.arpabo`\n```\n\\data\\\nngram 1=4\n\n\\1-grams:\n-1\tNO\n-1\tYES\n-99 <s>\n-1 </s>\n\n\\end\\\n```\nfstisstochastic命令则如同其名字的含义一样，用来检查G.fst是否是随机的。\n在`s5/data/lang`目录下会出现：\n1. `phones.txt`:将phone转换成数字。其中#0，#1是空字，用来表示句子的开头和结尾。<eps> 是一个特别的含义表示这个弧上没有符号。\n```\n<eps> 0\nSIL 1\nY 2\nN 3\n#0 4\n#1 5\n```\n2. `words.txt`\n```\n<eps> 0\n<SIL> 1\nNO 2\nYES 3\n#0 4\n<s> 5\n</s> 6\n```\n3. `L_disambig.fst, L.fs`t: the dict can be recognized by Kaldi\n4. `topo`: phone states transition(HMM)\n5. `oov`: out of vocabulary. 仅包含`<SIL>`\n6. ` phones`: some information about phones\n\n## 特征提取和训练\nMFCC特征提取和GMM-HMM建模\n\n提取梅尔倒谱系数\n```\nsteps/make_mfcc.sh --nj $num $input_dir $log_dir $output_dir\n```\n[语音信号处理（二）—— MFCC详解](https://zhuanlan.zhihu.com/p/60371062)\n梅尔倒谱系数是一种非线性的时频表示法，其应用了人耳对低频声音的听觉敏感度更高的原理。\n\n接着正则化倒谱特征\n```\nsteps/compute_cmvn_stats.sh \nutils/fix_data_dir.sh $input_dir\n```\n[Kaldi中的特征提取(二）- 特征变换](http://placebokkk.github.io/kaldi/2019/08/05/asr-kaldi-feat2.html)\n对训练集和测试集做同样的操作。\n这里我采用的参数是 $num=1 $output_dir=mfcc\n那么结果将会保存在mfcc文件夹下。\ncmvn_test_yesno.ark  cmvn_test_yesno.scp  cmvn_train_yesno.ark  cmvn_train_yesno.scp。\nark文件包含特征向量（用cat打开是乱码），scp文件是关系文件，从发音者到ark文件的对应。\n也可以直接跑脚本：\n```\nnum=1 \noutput_dir=mfcc\nfor x in train_yesno test_yesno; do\n  steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc  \n  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc        \n  utils/fix_data_dir.sh data/$x \ndone \n```\n## 单音素模型训练\n```\nsteps/train_mono.sh —nj $N —cmd $MAIN_CMD $DATA_DIR $LANG_DIR $OUTPUT_DIR\n```\n参数说明：\n—nj：job的数量，来自同一个speaker的语音不能并行处理。所以在本项目中只能选择为1。\n—cmd：为了使用本机的资源，调用”utils/run.pl”\n\n运行脚本：\n```\ntrain_cmd=\"utils/run.pl\" steps/train_mono.sh --nj 1 --cmd \"$train_cmd\" \\   \n--totgauss 400 \\   \ndata/train_yesno data/lang exp/mono0a\n```\n到现在我们已经完成了模型的训练。\n\n## 解码和测试\n\n接下来用测试集来验证一下模型的准确与否。\n第一步是创建一个全连接的FST网络。\n```\nutils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr\n```\n这个指令背后的脚本很长。基本上做的事情是创建以一个HCLG（HMM+Context+Lexicon+Grammer）的解码器并保存在exp/mono0a/graph_tgpr中。\n还记得我们的每一条语音都是8个连续的Ken或Lo吗(Ken，Lo是希伯来语的yes no）?训练好的模型的工作就是找出这8个词的顺序。\n`steps/decode.sh [options] <graph-dir> <data-dir> <decode-dir>`用来寻找每一个测试音频的最佳路径\n\n```\ndecode_cmd=\"utils/run.pl\" steps/decode.sh --nj 1 --cmd \"$decode_cmd\" \\\nexp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno \n```\n最后是查看结果的环节。\n在decode.sh内部最后会调用score.sh，这个脚本则生成预测的结果并且计算测试集Word error rate（WER）。  \n调用下列命令可以看到最好的效果：\n```\nfor x in exp/*/decode*; do [ -d $x ] && grep WER $x/wer_* | utils/best_wer.sh;\ndone\n```\n```\n%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10_0.0\n```\n解读一下结果，花了很长时间才弄懂这些东西是什么意思。\nWER后跟着的0.00是说字的错误率为0，即准确率为100%。\n测试集一共29条音频，每条音频有8个字（单音素的字）。一共232个字。\n参考Stanford的cs224s-17.lec04.pdf\n\n![WER的计算方法](https://upload-images.jianshu.io/upload_images/4328467-dc5c0fcaf24b3750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n![](https://upload-images.jianshu.io/upload_images/4328467-1c85e0672f7772cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n而wer结果文件中的10和0.0分别是lmwt(Language Weight)和wip(word insertion penalty，词插入惩罚)。lmwt用来平衡LM（语言模型）在多大程度上帮助AM（语音模型）![LMWT用途 \nhttps://skemman.is/bitstream/1946/31280/1/msc_anna_vigdis_2018.pdf](https://upload-images.jianshu.io/upload_images/4328467-3e2341231bb987fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n至于[wip](https://www.zhihu.com/question/31416764/answer/353868805)：\n> word insertion penalty, 简写WIP, 是HMM识别匹配过程中用于设置句长的一个参数，可以用来调节生成句子中的单词个数，当前主流的语音识别系统主要采用的都是音素识别，即根据单词的音标而不是单词来进行匹配，这就导致了，在识别过程中，可能很难确定单词的gap，如果让系统自由识别，根据参数初始化的模型来进行匹配的话有可能会生成一些诡异的由长单词构成的句子，或者有很多短单词构成的句子，这些匹配率很低的句子对HMM参数的优化作用很小，同时也很大概率会导致学习速率奇慢或者局部最优解这样的问题，所以通过设置这个参数来得到一些更符合语义结构的生成片段。\n\n最后，我们调用的所有脚本都在run.sh中。","tags":["Kaldi","ASR"]},{"title":"python程序同步webdav网盘（坚果云、owncloud)","url":"/2020/01/10/python程序同步webdav网盘（坚果云、owncloud）/","content":"\n在编写一个Django项目的时候有定时备份数据文件的需求。因为项目运行在云服务器中，担心的是服务器挂掉，所以备份的地方不能是同一台服务器。在这个项目里自己去建一个服务器来管理备份数据显得没有必要。目前网络存储提供商有许多家，我选取的方案是编写python定时程序连接支持WebDAV协议的网盘，例如坚果云和owncloud。\n\n分为两部分：\n\n1. Python同步代码编写\n2. Django定时任务编写\n\n## Python同步代码编写\n\n使用[webdavclient3](https://pypi.org/project/webdavclient3/)库来处理webDAV协议的部分。先安装：\n\n```\npip install webdavclient3\n```\n\n然后在自己的项目的某个地方建立一个py文件。我选的是`[项目目录]\\utils\\backup\\backup.py`\n\n编写python代码：\n\n```python\nfrom webdav3.client import Client\nfrom datetime import datetime\nfrom webdav3.exceptions import LocalResourceNotFound\n\nimport math\n# invoke this function every day.\ndef upload():\n    options = {\n        'webdav_hostname': \"网盘地址，如果是坚果云，不能只输入/dav/路径，似乎这个文件夹不能访问，在下面再建一个文件夹，比如backup。网址中需要有backup的地址比如https://dav.jianguoyun.com/dav/backup\",\n        'webdav_login': \"用户名\",\n        'webdav_password': \"密码，如果是坚果云填写应用密码\",\n        'disable_check': True, #有的网盘不支持check功能\n    }\n    client = Client(options)\n\t\t# 我选择用时间戳为备份文件命名\n    file_name = str(math.floor(datetime.now().timestamp())) + '.bak'\n    try:\n        # 写死的路径，第一个参数是网盘地址\n        client.upload('backup/' + file_name, '本地地址，绝对路径')\n        # 打印结果，之后会重定向到log\n        print('upload at ' + file_name)\n    except LocalResourceNotFound as exception:\n        print('An error happen: LocalResourceNotFound ---'  + file_name)\n\n# 如果是直接调用文件，执行upload()\nif __name__ == '__main__':\n    print('run upload')\n    upload()\n```\n\nPython的代码相对简短。只需要在服务器的命令行执行`python upload.py`，备份文件自动上传到网盘。然而还不完整，这个程序的目的是定期执行。接下来结合Django的定期执行接口做到每日备份。\n\n\n\n## Django定时任务编写\n\nDjango是目前很流行的python web服务框架。通过django自带的命令创建project和app（这一部分不讲）。\n\n当项目建好后，使用`python manage.py`可以运行项目、数据库代码整合。开发者可以通过继承BaseCommand类来自定义命令，然后再通过django_crontab定期执行命令。或者不通过自定义命令，直接使用django_crontab定期执行函数。\n\n### 创建backupCmd命令\n\n在之前创建的app下新建management/commands目录，在该目录下新建`backupCmd.py`\n\n```\nfrom django.core.management.base import BaseCommand\nfrom utils.backup.backup import upload\n\nclass Command(BaseCommand):\n    def handle(self, *args, **options):\n        upload()\n```\n\n当完成后，在项目根目录下执行`python manage.py backupCmd`就可以单次执行程序了。\n\n在setting.py尾部添加:\n\n```\n# 运行定时函数，每天1点运行。\nCRONJOBS = [\n    ('0 01 * * *', 'utils.backup.backup','>> ~/test_crontab.log')\n]\n```\n或\n```\n# 运行定时命令， \nCRONJOBS = [\n    ('*/1 * * * *', 'django.core.management.call_command', ['backupCmd'], {}, '>> ~/test_crontab.log'),\n]\n```\n\n然后执行`python manage.py crontab add`，定时任务加入其中。\n\n当时间到达的时候，程序将自动运行。日志会输出到`~/test_crontab.log`中。\n\nlinux中的定时任务crontab的语法如下:\n\n```\n*  *  *  *  * command\n分钟(0-59) 小时(0-23) 每个月的哪一天(1-31) 月份(1-12) 周几(0-6) shell脚本或者命令\n```\n\n\n\n对于使用到的网盘，希望能够付费支持一下。","tags":["Python"]},{"title":"用python写一个过滤器","url":"/2019/09/27/用python写一个过滤器/","content":"\n计算机视觉课Assigment2的内容.\n\n要求写出一个图像的过滤器出来。\n\n## 要求\n\n**Image Filtering.**Image filtering (or convolution) is a fundamental image processing tool. You will be writing your own function to implement image filtering from scratch. More specifically, you will implement my_imfilter()which imitates the filter2Dfunction in the OpenCV library. As specified in student_code.py, your filtering algorithm must\n\n(1) support grayscale and color images\n\n(2) support arbitrary shaped filters, as long as both dimensions are odd\n\n(e.g. 7x9 filters but not 4x5 filters)\n\n(3) pad the input image with zeros or reflected image content\n\n(4) return a filtered image which is the same resolution as the input image.\n\n使用numpy，PIL。\n\n## 步骤：\n\n1. 导入图像（不属于本次任务）\n2. 生成过滤核（不属于本次任务）\n3. padding\n4. calculate\n5. normalization（不属于本次任务，即假设过滤核和图像已经做好了正则化）\n6. 截断\n\n---\n\n## 函数定义\n\n图像过滤器的核心是一个function，函数的定义\n\n```python\ndef my_imfilter(image, filter)\n\treturn filtered_image\n```\n\n## 设计思路\n\n输入的图像为image，image可以为黑白或者RGB彩色。（channel=1 or channel=3)\n\n所以输入的image.ndim可能为2或者为3.\n\n第一部分代码需要判断黑白和彩色的情况.\n\n图片过滤器会将原图片的尺寸缩小,因为进行卷积操作是将原图像的一个矩阵框中的点和过滤核相乘后在求和得到新图像的一个点的值。\n\n【公式和图像】\n\n假设原始图像的高和宽为img_h, img_w，过滤核的大小为filter_h, filter_w。这里有一个条件过滤核的长宽都必须为为奇数。（为什么呢？）那么除掉最中心的像素外，过滤核可以被分为四等份。每一部分的长宽(padding)为 pad_h = (filter_h-1)/2和pad_w = (filter_w-1)/2\n\n那么输出的尺寸为img_h - pad_h * 2 和 img_w - pad_w *2，因为图像两边都需要减去pad_h的尺寸。\n\n同时我们还要给过滤器做一个上下左右变换。因为卷积操作和相关操作的过滤核移动方向是相反的，应该是从下往上，从右往左。\n\n## 实例代码：\n\n```python\ndef my_imfilter(image, filter):\n  \"\"\"\n  Apply a filter to an image. Return the filtered image.\n\n  Args\n  - image: numpy nd-array of dim (m, n, c)\n  - filter: numpy nd-array of dim (k, k)\n  Returns\n  - filtered_image: numpy nd-array of dim (m, n, c)\n\n  HINTS:\n  - You may not use any libraries that do the work for you. Using numpy to work\n   with matrices is fine and encouraged. Using opencv or similar to do the\n   filtering for you is not allowed.\n  - I encourage you to try implementing this naively first, just be aware that\n   it may take an absurdly long time to run. You will need to get a function\n   that takes a reasonable amount of time to run so that the TAs can verify\n   your code works.\n  - Remember these are RGB images, accounting for the final image dimension.\n  \"\"\"\n\n  assert filter.shape[0] % 2 == 1\n  assert filter.shape[1] % 2 == 1\n  # 默认设置图像的通道数，如果只有一个通道，即输入图片是二维的。\n  channel = 1\n  # 如果输入维度=3，通道数等于第三个维度的元素数量\n  if image.ndim == 3:\n    channel = image.shape[2]\n  # 获取图片的长度和宽度\n  image_h, image_w = image.shape[:2]\n  # 将过滤核做上下、左右翻转，以确保是卷积操作而不是相关操作\n  filter = np.flipud(filter)\n  filter = np.fliplr(filter)\n  # 获取过滤核的长度和宽度\n  filter_h, filter_w = filter.shape\n  pad_h = (filter_h - 1) // 2\n  pad_w = (filter_w - 1) // 2\n  # 先扩充原图像,为了不影响原图像，需要复制一份图像\n  image_cp = image.copy()\n  image_cp = np.pad(image_cp,[(pad_h,pad_h),(pad_w,pad_w),(0,0)],\"constant\")\n  # 生成过滤后的图片的空容器，尺寸可以是原本的尺寸，在这里为了下面坐标转换方便，扩大了长宽。\n  filtered_image = np.zeros(image_cp.shape)\n  # 第一层是对于不同的channel做卷积  \n  for i in range(channel):\n    # 第二层是高度y轴像素遍历\n    for j in range(pad_h,image_h+pad_h):\n        # 第三层是宽度x轴像素遍历\n        for k in range(pad_w,image_w+pad_w):\n            # 算法核心，加上上面的翻转式卷积操作，单独来看是相关操作。其实可以通过 step=-1来做\n            filtered_image[j,k,i] = np.sum(np.multiply(image_cp[j-pad_h:j+pad_h+1,k-pad_w:k+pad_w+1,i],filter))\n\n  return filtered_image[pad_h:image_h+pad_h, pad_w:image_w+pad_w,:]\n```\n\n\n\n---\n\n相关链接：\n\n[卷积与互相关的一点探讨](https://zhuanlan.zhihu.com/p/33194385)\n\n","tags":["Python"]},{"title":"我为什么决定读研究生","url":"/2019/09/26/我为什么决定读研究生/","content":"\n在这个时刻，为了研招网确认报名而未睡，想想写写自己为什么从一个就业党到保研的路。\n\n我从很久以前就“认清事实”，自己的学习能力只能算是中游偏下，自律和耐力也不行。\n\n本来想着自己这不高的GPA是没有可能继续读书的，别说保研了，连考研，出国等等继续读书的可能都一一被自己否决了。还好，有编程的能力，对代码还算喜欢。本来在华为实习，基本拿到了offer。跟父母也说了自己去就业有什么不好的，拿着一份很有吸引力的薪水，做着自己也算是喜欢的编写代码的工作。\n\n对我而言考研是不可能的。因为我害怕考研的失败和浪费自己生命的一年时间。\n\n后来在本科导师的建议下试着投递保研申请，没想到过程是意外的轻松，也许是因为南科大的计算机系2/5左右的人都有申请出国或者就业的打算吧。\n\n所以第一个条件满足了。但是我心里是犹豫的，因为华为的offer最终还没下来，想着一边是马上就能赚钱，购置自己喜欢的商品，另外一边是3年的研究生生活，只有奖学金的收入。\n\n我这个人对钱很在乎的。\n\n但是我其实是对现在进入工作抱有疑虑的，我觉得如果马上进入工作，可能很快我的生活会失衡，因为我自己的生活单调，如果再在安静的园区里工作，没有人脉，没有社交活动。生活会很枯燥的。在实习中工作的状态不是我想要的。起码工作以外很少有与人沟通交流的机会。\n\n最终走上了读研究生这条道路，只是我应该警觉研究生3年不是一个避风港，而是自己意识到不足而该改进的一个宝贵的成长的，准备将来进入社会的一个阶段。","tags":["个人"]},{"title":"用阿里云服务器自己搭建2do同步caldav服务器","url":"/2019/09/23/用阿里云服务器自己搭建2do同步caldav服务器/","content":"\n\n\n## 用阿里云服务器自己搭建2do同步caldav服务器\n一个月前，把任务管理从Microsoft TO-DO迁移到2Do中，自己手上常用一台小米9和Macbook Pro，同步成了问题，不能使用icloud remainder。那就只有DropBox等服务可用。\n因为Dropbox在国内不稳定的连接以及近阶段自己手上的梯子挂掉了，所以琢磨自己搭一个同步使用的服务器。\n流程非常简单。一个小时以内就可办完。\n使用到的:\n1. 阿里云ECS低配版，如果使用学生版一个月9.5元。\n2. python3。\n3. radicale，使用python3编写的caldav服务器程序。\n\n### 步骤一览: \n1. 云服务器购买与登录\n2. 安全组设置\n3. Python3环境\n4. 安装radicale\n5. 配置radicale\n\n---\n\n### 云服务器准备\n\n购买与登录的流程就自己搜索吧。我购买的是Ubunt的主机，也可以使用CentOS。\n\n需要注意的是在创建服务器实例之前，最好先将本机的ssh public key导入到阿里云中，并且新建实例后选择该public key，不然不能够在终端中登录，而阿里云提供的浏览器vnc完全不能用。也可以在创建实例后再创建public key，只是需要重启实例罢了。\n\n### 安全组设置\n\n阿里云默认关闭大多数端口，包括了我们这次要使用到的5232端口。所以请自行搜索“阿里云 安全组 端口开启“完成该步骤。\n\n\n\n### Python3的环境搭建\n\n这一步大体上也是请搜索“Ubuntu python3“\n\n需要注意的是先使用`sudo apt-get update`更新本地软件目录。\n\n\n\n### 安装radicale\n\n这一步会很详细的讲。\n\n安装radicale\n\n```\npython3 -m pip install --upgrade radicale \n```\n若是在本地使用，可以使用下面这个命令。\n\n```shell\npython3 -m radicale --config \"\" --storage-filesystem-folder=~/.var/lib/radicale/collections\n```\n\n然后访问 http://localhost:5232\n\n但是若在服务器上访问就无法访问。需要配置参数。\n\n我使用的配置有，把配置文件放在`/etc/radicale/config`\n\n```\n[server]\n# Bind all addresses\nhosts = 0.0.0.0:5232\ndaemon = True\n[auth]\ntype = htpasswd\nhtpasswd_filename = /etc/radicale/users\nhtpasswd_encryption = plain\n[storage]\nfilesystem_folder = ~/.var/lib/radicale/collections\n```\n\n想详细了解更多配置选项可以查看[configuration](https://radicale.org/configuration/)\n\n然后可以设置用户名和密码, 把配置文件放在`/etc/radicale/users`\n\n```\nuser:password\n```\n\n然后启动就好了。\n\n```\npython3 -m radicale\n```\n\n正常访问。\n\n然后在2Do的配置上选择就好了。\n\n![2do](/images/2do.png)","tags":["Python","Linux"]},{"title":"如何测试那些难以测试的方法？(stdin/stdout标准输入输出，文件，main方法，private私有方法)","url":"/2019/09/23/如何测试那些难以测试的方法/","content":"\n\n\n\n\n## 如何测试标准输出中的内容\n\n标准输出：`System.out.println()`\n\n方法流程：\n1. 使用`OutputStream, System.setOut`重定向输出流\n2. 使用`System.getProperty(\"line.separator)`来正确的测试下一行\n3. 使用`System.setOut, System.out`恢复输出流\n---\n\n### 样例代码\n\n```java\n// PrintClass.java\npublic class PrintClass {\n\tpublic void printSome() {\n\t\tSystem.out.println(\"Just follow your heard\");\n\t}\n}\n// PrintClassTest.java,忽略掉import的部分\npublic class PrintClassTest {\n\t@Test\n\tpublic void testPrintSome() {\n\t\t// 重定向标准输出到指定printstream中\n\t\tOutputStream os = new ByteArrayOutputStream();\n\t\tPrintStream ps = new PrintStream(os);\n\t\tSystem.setOut(ps);\n\t\t// 执行测试代码\n\t\tPrintClass pc = new PrintClass();\n\t\tpc.printSome();\n\t\tassertEquals(\"Just follow your heard\"\n\t\t\t\t\t\t+ System.getProperty(\"line.separator\"),\n                os.toString());\n\t\t// 恢复重定向\n\t\tPrintStream originalOut = System.out;\n    \tSystem.setOut(originalOut);\n\t}\n}\n\n```\n\n完整代码在：[week2:printtest](https://github.com/chenminken/software-testing/tree/master/week2/printtest)\n\n## 如何测试私有方法\n\nJava中的私有方法不能通过obj.method()的方式来调用。如果需要测试私有方法怎么办？\n\n使用反射机制获取这个私有方法。\n\n需要使用到的方法`Class.getDeclaredMethod(), Method.setAccessible(), Method.invoke()`\n\n方法流程：\n\n1. 获取需要测试类的class对象（可以使用class.forname的方法，也可以直接使用[ClassName].class的方法）\n2. 获取该class对象指定的私有方法的Method对象（`[classobj.class].getDeclaredMethod(String name, Class<?>... parameterTypes)`）\n3. 修改私有方法的访问性为公开访问(`[MethodObj].setAccessible(true)`。\n4. 实例化需要测试类的object对象(`[ClassName.class].newInstance()`)\n5. 调用该私有方法测试(`[MethodObj].invoke([objectInstance], ...)`)\n\n---\n\n### 样例代码\n\n```java\n// PrivateMethodClass.java\npublic class PrivateMethodClass {\n\tprivate int privatePlus(int a, int b) {\n\t\treturn a + b;\n\t}\n}\n// PrivateMethodClassTest.java\npublic class PrivateMethodClassTest {\n    @Test\n    public void testPrivatePlus1() throws NoSuchMethodException, ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException {\n        Class cls = Class.forName(\"PrivateMethodClass\");\n        Method msd = cls.getDeclaredMethod(\"privatePlus\", int.class, int.class);\n        msd.setAccessible(true);\n        Object obj = cls.newInstance();\n        int res = (Integer)msd.invoke(obj, 4,6);\n        assertEquals(10, res);\n    }\n}\n```\n\n完整代码在：[week2:privateTest](https://github.com/chenminken/software-testing/tree/master/week2/privateTest)","tags":["软件测试","Java"]},{"title":"软件测试week2 note","url":"/2019/09/16/软件测试week2-note/","content":"\n\n\n## Fault, failure and error\n\nexample\n\n\n\n## 为什么软件测试变得越来越重要？\n\n\n\n## 测试过程的成熟等级\n\nLevel 0\n\nLevel 1\n\nLevel 2\n\nLevel 3\n\nLevel 4\n\n\n\n## 每一个测试的目的是什么？\n\n\n\n\n\n## 什么时候开始测试？\n\n\n\n\n\n## 不测试的代价？\n\n\n\n\n\n## 我们为何要测试软件？\n\n\n\n## 软件活动中的测试等级\n\n\n\n## 软件的可测试性（Software Testability）\n\n\n\n## 可观测性和可控制性（Observability and Controllability）\n\n\n\n## 一个测试样例的组成部分\n\n\n\n\n\n## 自动化测试框架\n\nJunit\n\n\n\n## Hamcrest Common Matchers \n\n\n\n## 如何测试标准输出中的字符串？\n\n## 如何测试Main方法？"},{"title":"Machine Learning Week1 Note","url":"/2019/09/13/Machine-Learning-Week1-Note/","content":"\n此系列博文是学习Coursera上Stanford的《Machine Learning》MOOC的笔记。\n\n课程共分为11周，大约需要56小时学习。\n\n此文是第一周的笔记。\n\n## 学习动机\n\n本课程是在同学的推荐下学习的，因为大四的两个项目都需要用到机器学习的知识和工具，所以需要先对机器学习有一个大概的了解。这门课程是很有名的。且Coursera上的评分为4.9，有250多万人注册过这门课！\n\nNg的课程说明中对本门课程的定义为不仅\n\n## 介绍\n\n机器学习的应用有：\n\n1. 垃圾邮件拦截。\n2. 网页搜索。\n3. 电子相册的面孔识别\n\n机器学习适用的场景：代码不能手动编写出来。比如：\n\n1. 直升机自动驾驶\n2. 自然语言识别\n3. 视觉识别\n\n机器学习在数据挖掘中的应用。\n\n推荐系统，点击流。\n\n## 什么是机器学习？\n\n公认的机器学习的定义有两种说法。一种是Mitchell提出的ETP定义，一种是\n\nArthur Samuel提出的学科定义。\n\n### ETP（Experience, Task, Performance）\n\n> A computer is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n\nExperience，实验经历。\n\nTask：任务。\n\nPerformance：预测的准确度。\n\n### Samuel的学科定义\n\n> Machine Learning is the field of study gives computers that ability to learn without being explicitly programmed.\n\n机器学习是研究赋予不用显性的编写代码就能使计算机获得学习的能力的学科。\n\n## 监督学习\n\n监督学习是指输入训练特征数据，并且告诉你正确的答案是什么，然后再输入未知的特征，通过对之前的数据挖掘出规律预测未知输入对应的输出。\n\n监督学习需要使用标记好的特征数据训练。\n\n监督学习可分为两种形式。一种是分类（Classification），分类的结果是离散有限的。另外一种是回归（regression），回归的输出结果是连续的。以卖房为例，分类的结果可以是预测指定房子在未来三个月是否能卖出去。而回归的结果可以是符合指定条件（比如面积和地段）的房子的预期价格。价格既可以是10000，也可以是10001.23。\n\n在上面的例子中，面积和地段是学习模型的两个特征，除了它们以外，可能的特征还有小区绿化率、1公里内的地铁站的数量等等特征。如果需要，甚至可以使用近乎无穷数量的特征进行监督学习。\n\n## 非监督学习\n\n非监督学习不需要知道输入数据的标签和分类，而是通过输入数据的结构聚类（Clustering）。比如google新闻推荐，自动以一个新闻事件为主题，聚集相关的新闻报道。或者社交网络中人群分类都可以使用非监督学习进行。\n\n课程中举了一个Cokcktail Party Problem的问题。即通过两个不同位置的声源分离出环境音和人声。\n\n**TODO：实现一个算法**\n\n","tags":["Machine Learning","Coursera"]},{"title":"阅读日报20190911","url":"/2019/09/11/阅读日报20190911/","content":"\n# 神经网络\n\n[神经网络浅讲：从神经元到深度学习](https://www.cnblogs.com/subconscious/p/5058741.html)\n\n对于神经网络，大神级别的导读和总结。\n\n不说了，去抄文中提到的概念去了。\n\n话说软件工程的项目用到的车牌识别技术EasyPR还是此文作者开发的呀。\n\n\n\n","tags":["阅读日报"]},{"title":"重建博客","url":"/2019/09/08/重建博客/","content":"\n2019年4月的时候用hexo和github page创建过一个博客，但是在更换电脑的时候意外丢失了模版。后又恢复回来，只剩下了网页，，没有网站配置和md原始文件。\n\n"}]